{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TMEN5aRQwtBa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import librosa\n",
        "from scipy.fftpack import dct\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ekstraksi fitur menggunakan MFCC"
      ],
      "metadata": {
        "id": "mYtzBpcj3TQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi ekstraksi MFCC\n",
        "def dc_removal(signal):\n",
        "    mean = np.mean(signal)\n",
        "    return signal - mean\n",
        "\n",
        "def pre_emphasis(signal):\n",
        "    pre_emphasis = 0.97\n",
        "    signal_emphasis = np.append(signal[0], signal[1:] - pre_emphasis * signal[:-1])\n",
        "    return signal_emphasis\n",
        "\n",
        "def frame_blocking(signal_emphasis, sample_rate):\n",
        "    frame_size = 0.025\n",
        "    frame_stride = 0.01\n",
        "    frame_length = int(frame_size * sample_rate)\n",
        "    frame_step = int(frame_stride * sample_rate)\n",
        "    signal_length = len(signal_emphasis)\n",
        "    frames_overlap = frame_length - frame_step\n",
        "\n",
        "    num_frames = (np.abs(signal_length - frames_overlap) // np.abs(frame_length - frame_step)).astype(int)\n",
        "    rest_samples = np.abs(signal_length - frames_overlap) % np.abs(frame_length - frame_step)\n",
        "\n",
        "    pad_signal_length = int(frame_length - rest_samples)\n",
        "    z = np.zeros((pad_signal_length))\n",
        "    pad_signal = np.append(signal_emphasis, z)\n",
        "\n",
        "    indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + np.tile(\n",
        "        np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)\n",
        "    ).T\n",
        "\n",
        "    frames = pad_signal[indices.astype(np.int32, copy=False)]\n",
        "    return frames, frame_length\n",
        "\n",
        "def windowing(frames, frame_length):\n",
        "    frames = frames * (np.hamming(frame_length))\n",
        "    return frames\n",
        "\n",
        "def fft(frames):\n",
        "    NFFT = 512\n",
        "    mag_frames = np.absolute(np.fft.rfft(frames, NFFT))\n",
        "    pow_frames = ((1.0 / NFFT) * ((mag_frames) ** 2))\n",
        "    return pow_frames, NFFT\n",
        "\n",
        "def filter_bank(pow_frames, sample_rate, NFFT):\n",
        "    nfilt = 40\n",
        "    low_freq_mel = 0\n",
        "    high_freq_mel = (2595 * np.log10(1 + (sample_rate / 2) / 700))  # Convert Hz to Mel\n",
        "    mel_points = np.linspace(low_freq_mel, high_freq_mel, nfilt + 2)  # Equally spaced in Mel scale\n",
        "    hz_points = (700 * (10 ** (mel_points / 2595) - 1))  # Convert Mel to Hz\n",
        "    bin = np.floor((NFFT + 1) * hz_points / sample_rate)\n",
        "\n",
        "    fbank = np.zeros((nfilt, int(np.floor(NFFT / 2 + 1))))\n",
        "    for m in range(1, nfilt + 1):\n",
        "        f_m_minus = int(bin[m - 1])  # left\n",
        "        f_m = int(bin[m])  # center\n",
        "        f_m_plus = int(bin[m + 1])  # right\n",
        "\n",
        "        for k in range(f_m_minus, f_m):\n",
        "            fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n",
        "        for k in range(f_m, f_m_plus):\n",
        "            fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\n",
        "\n",
        "    filter_banks = np.dot(pow_frames, fbank.T)\n",
        "    filter_banks = np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)  # Numerical Stability\n",
        "    filter_banks = 20 * np.log10(filter_banks)  # dB\n",
        "\n",
        "    return (filter_banks / np.amax(filter_banks)) * 255\n",
        "\n",
        "def cepstral_liftering(filter_banks):\n",
        "    num_ceps = 12\n",
        "    cep_lifter = 11\n",
        "    mfcc = dct(filter_banks, type=2, axis=1, norm='ortho')[:, :(num_ceps)]\n",
        "    (nframes, ncoeff) = mfcc.shape\n",
        "    n = np.arange(ncoeff)\n",
        "    lift = 1 + (cep_lifter / 2) * np.sin(np.pi * n / cep_lifter)\n",
        "    mfcc = (np.mean(mfcc, axis=0) + 1e-8)\n",
        "    return mfcc\n",
        "\n",
        "def extract_mfcc(file_path):\n",
        "    try:\n",
        "        # Baca file audio WAV\n",
        "        audio_data, sample_rate = librosa.load(file_path, sr=None)\n",
        "\n",
        "        # DC Removal\n",
        "        dc_removed_signal = dc_removal(audio_data)\n",
        "\n",
        "        # Pre-Emphasis\n",
        "        pre_emphasized_signal = pre_emphasis(dc_removed_signal)\n",
        "\n",
        "        # Frame Blocking\n",
        "        frames, frame_length = frame_blocking(pre_emphasized_signal, sample_rate)\n",
        "\n",
        "        # Windowing\n",
        "        windowed_frames = windowing(frames, frame_length)\n",
        "\n",
        "        # FFT, Mel Frequency Wrapping, dan DCT\n",
        "        pow_frames, NFFT = fft(windowed_frames)\n",
        "        filter_banks = filter_bank(pow_frames, sample_rate, NFFT)\n",
        "        mfcc_features = cepstral_liftering(filter_banks)\n",
        "\n",
        "        return mfcc_features\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def pad_mfcc_features(features, max_length):\n",
        "    num_features = len(features)\n",
        "    feature_shape = features[0].shape[1] if len(features[0].shape) > 1 else 1\n",
        "\n",
        "    # Menyesuaikan panjang fitur tanpa DTW\n",
        "    padded_features = np.zeros((num_features, max_length, feature_shape))\n",
        "\n",
        "    for i, feature in enumerate(features):\n",
        "        feature_length = len(feature)\n",
        "\n",
        "        if feature_length < max_length:\n",
        "            # Membuat pad dengan menggunakan nilai nol\n",
        "            padded_feature = np.zeros((max_length, feature_shape))\n",
        "            padded_feature[:feature_length, :] = feature  # Mengisi dengan fitur yang sebenarnya\n",
        "            padded_features[i] = padded_feature\n",
        "        else:\n",
        "            # Jika panjang fitur sudah cukup, gunakan fitur tanpa modifikasi\n",
        "            padded_features[i, :feature_length, :] = feature.reshape((feature_length, feature_shape))  # Reshape here\n",
        "\n",
        "    return padded_features"
      ],
      "metadata": {
        "id": "1zRRDbf6SUET"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset di representasikan dan dilakukan pelatihan model terhadap data latih dengan svm. Pada tahap ini data latih melewati tahap ekstraksi ciri lalu hasil tersebut akan dilakukan pelatihan model dengan SVM\n"
      ],
      "metadata": {
        "id": "CizTVUJf3gbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(base_folder, labels):\n",
        "    # Mendapatkan dataset\n",
        "    dataset = {\"features\": [], \"labels\": []}\n",
        "\n",
        "    for label in labels:\n",
        "        folder_path = os.path.join(base_folder, label)\n",
        "        files = os.listdir(folder_path)\n",
        "\n",
        "        for file in files:\n",
        "            file_path = os.path.join(folder_path, file)\n",
        "            features = extract_mfcc(file_path)\n",
        "\n",
        "            if features is not None:\n",
        "                dataset[\"features\"].append(features)\n",
        "                dataset[\"labels\"].append(label)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base_folder = \"/content/drive/MyDrive/Dataset EKG /Data Latih EKG\"\n",
        "\n",
        "    labels = [\"extrastole\", \"normal\", \"murmur\"]\n",
        "    print(f\"Analisis Kinerja MFCC dan SVM untuk Klasifikasi Aritmia Jantung\")\n",
        "    # Mendapatkan dataset\n",
        "    dataset = get_dataset(base_folder, labels)\n",
        "\n",
        "    # Normalisasi panjang fitur MFCC tanpa DTW\n",
        "    max_length = max(len(feature) for feature in dataset[\"features\"])\n",
        "\n",
        "    # Sisipkan padding pada setiap sampel\n",
        "    padded_features = pad_mfcc_features(dataset[\"features\"], max_length)\n",
        "\n",
        "    # Ubah matriks dua dimensi\n",
        "    num_samples, max_length, num_features = padded_features.shape\n",
        "    reshaped_features = padded_features.reshape((num_samples, max_length * num_features))\n",
        "\n",
        "    # Normalisasi fitur MFCC menggunakan StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "    reshaped_features_scaled = scaler.fit_transform(reshaped_features)\n",
        "\n",
        "    # Menggunakan pipeline untuk menggabungkan normalisasi dan klasifikasi\n",
        "    model = make_pipeline(StandardScaler(), OneVsRestClassifier(SVC(kernel='rbf', C=20, gamma=0.01)))\n",
        "\n",
        "    # Melatih model\n",
        "    model.fit(reshaped_features_scaled, dataset[\"labels\"])\n",
        "\n",
        "    print(\"\\n----------------Model Training Information----------------\")\n",
        "    print(f\"Kernel: {model.named_steps['onevsrestclassifier'].estimators_[0].kernel}\")\n",
        "    print(f\"C: {model.named_steps['onevsrestclassifier'].estimators_[0].C}\")\n",
        "    print(f\"Gamma: {model.named_steps['onevsrestclassifier'].estimators_[0].gamma}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ucy4g2YGSrUi",
        "outputId": "245c6772-d139-4402-a5f9-ab0e45db411b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analisis Kinerja MFCC dan SVM untuk Klasifikasi Aritmia Jantung\n",
            "\n",
            "----------------Model Training Information----------------\n",
            "Kernel: rbf\n",
            "C: 20\n",
            "Gamma: 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setelah model SVM dari data latih didapat maka dilanjutkan dengan menguji kinerja model pada data uji"
      ],
      "metadata": {
        "id": "re8C-ap733kn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # Evaluasi model menggunakan data uji\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    unique_samples = set()\n",
        "\n",
        "    for label in labels:\n",
        "        test_folder = os.path.join(\"/content/drive/MyDrive/Dataset EKG /Data Uji EKG\", label)\n",
        "        files = os.listdir(test_folder)\n",
        "\n",
        "        for file in files:\n",
        "            test_audio_file = os.path.join(test_folder, file)\n",
        "            # print(f\"Test Audio File: {test_audio_file}\")\n",
        "\n",
        "            # Ekstraksi fitur MFCC\n",
        "            mfcc_features_test = extract_mfcc(test_audio_file)\n",
        "\n",
        "            if mfcc_features_test is not None:\n",
        "                # Menampilkan audio waveform\n",
        "                sample_rate, audio_data_test = librosa.load(test_audio_file, sr=None)\n",
        "\n",
        "                # Normalisasi panjang fitur MFCC pada data uji\n",
        "                mfcc_features_test = pad_mfcc_features([mfcc_features_test], max_length)[0]\n",
        "\n",
        "                # Ratakan dimensi kedua dan ketiga dari fitur MFCC\n",
        "                mfcc_features_test_flat = mfcc_features_test.reshape((1, -1))\n",
        "\n",
        "                # Normalisasi fitur MFCC uji menggunakan StandardScaler\n",
        "                mfcc_features_test_flat_scaled = scaler.transform(mfcc_features_test_flat)\n",
        "\n",
        "                # Prediksi label menggunakan model yang telah dilatih\n",
        "                recognized_label = model.predict(mfcc_features_test_flat_scaled)\n",
        "\n",
        "                # Uji akurasi\n",
        "                total_samples += 1\n",
        "                true_labels.append(label)\n",
        "                predicted_labels.append(recognized_label[0])\n",
        "                unique_samples.add(file)\n",
        "\n",
        "                # Perbarui correct_predictions\n",
        "                if recognized_label[0] == label:\n",
        "                    correct_predictions += 1\n",
        "\n",
        "    # Hitung total sampel unik\n",
        "    total_samples = len(unique_samples)"
      ],
      "metadata": {
        "id": "IEwr5-BTSx_X"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluasi dan optimasi\n"
      ],
      "metadata": {
        "id": "fer8PZt23Jxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # Menghitung metrik evaluasi\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "    recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "    print(f\"\\n----------------Model Evaluation----------------\")\n",
        "    print(f\"Total Samples: {total_samples}\")\n",
        "    print(f\"Correct Predictions: {correct_predictions}\")\n",
        "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpeolRJOkAd6",
        "outputId": "54d0ccf1-300d-4c94-ed83-83f043c0b09c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----------------Model Evaluation----------------\n",
            "Total Samples: 45\n",
            "Correct Predictions: 33\n",
            "Accuracy: 73.33%\n",
            "Precision: 0.76\n",
            "Recall: 0.73\n",
            "F1-Score: 0.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simpan dataframe dalam bentuk csv"
      ],
      "metadata": {
        "id": "-hwxJfHd4ZkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    df = pd.DataFrame({'True_Label': true_labels, 'Predicted_Label': predicted_labels})\n",
        "    df_new = pd.concat([df, pd.DataFrame(reshaped_features_scaled)], axis=1)\n",
        "    df_new.to_csv(\"/content/drive/MyDrive/Dataset EKG /df_result.csv\", index=False)"
      ],
      "metadata": {
        "id": "w4Ika-pRS2Qu"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QSatEMqk_os",
        "outputId": "57af19bc-ef7e-43e1-b1ad-998bccbf8bf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python_speech_features"
      ],
      "metadata": {
        "id": "QW-f5HiW0bi3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b796436-ccb3-4301-c43f-573e62421d0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python_speech_features\n",
            "  Downloading python_speech_features-0.6.tar.gz (5.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: python_speech_features\n",
            "  Building wheel for python_speech_features (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python_speech_features: filename=python_speech_features-0.6-py3-none-any.whl size=5869 sha256=37941ad97c4bcbf88afd7afdeaecd510a2e0938022614f2001d7af89954cc579\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/9e/68/30bad9462b3926c29e315df16b562216d12bdc215f4d240294\n",
            "Successfully built python_speech_features\n",
            "Installing collected packages: python_speech_features\n",
            "Successfully installed python_speech_features-0.6\n"
          ]
        }
      ]
    }
  ]
}